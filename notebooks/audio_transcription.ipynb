{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for this project is the Common Voice dataset. Common Voice is a massive multi-lingual corpus of read speech by Mozilla [1]. This project used Common Voice Corpus 20.0 subset for Indonesian language.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/cv-corpus-6.1-indonesian/train.tsv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accent</th>\n",
       "      <th>locale</th>\n",
       "      <th>segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4c81270f49ada076d376a968994e1533674531b0fae896...</td>\n",
       "      <td>common_voice_id_19192526.mp3</td>\n",
       "      <td>Kamar adik laki-laki saya lebih sempit daripad...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>twenties</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>id</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4c81270f49ada076d376a968994e1533674531b0fae896...</td>\n",
       "      <td>common_voice_id_19192527.mp3</td>\n",
       "      <td>Ayah akan membunuhku.</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>twenties</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>id</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4c81270f49ada076d376a968994e1533674531b0fae896...</td>\n",
       "      <td>common_voice_id_19192528.mp3</td>\n",
       "      <td>Ini pulpen.</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>twenties</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>id</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c81270f49ada076d376a968994e1533674531b0fae896...</td>\n",
       "      <td>common_voice_id_19192535.mp3</td>\n",
       "      <td>Akira pandai bermain tenis.</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>twenties</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>id</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4c81270f49ada076d376a968994e1533674531b0fae896...</td>\n",
       "      <td>common_voice_id_19192536.mp3</td>\n",
       "      <td>Dia keluar dari ruangan tanpa mengatakan sepat...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>twenties</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>id</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           client_id  \\\n",
       "0  4c81270f49ada076d376a968994e1533674531b0fae896...   \n",
       "1  4c81270f49ada076d376a968994e1533674531b0fae896...   \n",
       "2  4c81270f49ada076d376a968994e1533674531b0fae896...   \n",
       "3  4c81270f49ada076d376a968994e1533674531b0fae896...   \n",
       "4  4c81270f49ada076d376a968994e1533674531b0fae896...   \n",
       "\n",
       "                           path  \\\n",
       "0  common_voice_id_19192526.mp3   \n",
       "1  common_voice_id_19192527.mp3   \n",
       "2  common_voice_id_19192528.mp3   \n",
       "3  common_voice_id_19192535.mp3   \n",
       "4  common_voice_id_19192536.mp3   \n",
       "\n",
       "                                            sentence  up_votes  down_votes  \\\n",
       "0  Kamar adik laki-laki saya lebih sempit daripad...         2           0   \n",
       "1                              Ayah akan membunuhku.         2           0   \n",
       "2                                        Ini pulpen.         2           0   \n",
       "3                        Akira pandai bermain tenis.         2           0   \n",
       "4  Dia keluar dari ruangan tanpa mengatakan sepat...         2           1   \n",
       "\n",
       "        age gender  accent locale  segment  \n",
       "0  twenties   male     NaN     id      NaN  \n",
       "1  twenties   male     NaN     id      NaN  \n",
       "2  twenties   male     NaN     id      NaN  \n",
       "3  twenties   male     NaN     id      NaN  \n",
       "4  twenties   male     NaN     id      NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df[\"up_votes\"] >= train_df[\"down_votes\"]]\n",
    "train_df[\"sentence\"] = train_df[\"sentence\"].str.lower()\n",
    "train_df = train_df[[\"path\", \"sentence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path        0\n",
       "sentence    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../data/cv-corpus-6.1-indonesian/test.tsv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accent</th>\n",
       "      <th>locale</th>\n",
       "      <th>segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>057bf45c0c338db897f5717f744bcac8a2ac2eee990a42...</td>\n",
       "      <td>common_voice_id_22888800.mp3</td>\n",
       "      <td>Minggu depan kakak perempuan saya menikah.</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>id</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0835fbbf1d609a6ed421eef134a48ff06d719121b41f3b...</td>\n",
       "      <td>common_voice_id_24015257.mp3</td>\n",
       "      <td>Berbagai bahasa daerah dan bahasa asing menjad...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>id</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0c8ac0307f35c73b09d8fc0d92e4c183e3078adee87212...</td>\n",
       "      <td>common_voice_id_24015280.mp3</td>\n",
       "      <td>apa yang bisa saya berikan kepadamu?</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>id</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19285f8e012ad31cad237d53bab348ce59a5cc13684754...</td>\n",
       "      <td>common_voice_id_20425643.mp3</td>\n",
       "      <td>Inilah dunia kecil.</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>id</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3502377c5fb712169a3f2fe5583906e4b3a5ecba27bf2c...</td>\n",
       "      <td>common_voice_id_22185104.mp3</td>\n",
       "      <td>nol</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>id</td>\n",
       "      <td>Benchmark</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           client_id  \\\n",
       "0  057bf45c0c338db897f5717f744bcac8a2ac2eee990a42...   \n",
       "1  0835fbbf1d609a6ed421eef134a48ff06d719121b41f3b...   \n",
       "2  0c8ac0307f35c73b09d8fc0d92e4c183e3078adee87212...   \n",
       "3  19285f8e012ad31cad237d53bab348ce59a5cc13684754...   \n",
       "4  3502377c5fb712169a3f2fe5583906e4b3a5ecba27bf2c...   \n",
       "\n",
       "                           path  \\\n",
       "0  common_voice_id_22888800.mp3   \n",
       "1  common_voice_id_24015257.mp3   \n",
       "2  common_voice_id_24015280.mp3   \n",
       "3  common_voice_id_20425643.mp3   \n",
       "4  common_voice_id_22185104.mp3   \n",
       "\n",
       "                                            sentence  up_votes  down_votes  \\\n",
       "0         Minggu depan kakak perempuan saya menikah.         2           0   \n",
       "1  Berbagai bahasa daerah dan bahasa asing menjad...         2           1   \n",
       "2               apa yang bisa saya berikan kepadamu?         2           0   \n",
       "3                                Inilah dunia kecil.         2           1   \n",
       "4                                                nol         2           0   \n",
       "\n",
       "   age gender  accent locale    segment  \n",
       "0  NaN    NaN     NaN     id        NaN  \n",
       "1  NaN    NaN     NaN     id        NaN  \n",
       "2  NaN    NaN     NaN     id        NaN  \n",
       "3  NaN    NaN     NaN     id        NaN  \n",
       "4  NaN    NaN     NaN     id  Benchmark  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[test_df[\"up_votes\"] >= test_df[\"down_votes\"]]\n",
    "train_df[\"sentence\"] = train_df[\"sentence\"].str.lower()\n",
    "test_df = test_df[[\"path\", \"sentence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path        0\n",
       "sentence    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df, test_df = train_test_split(test_df, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preprocessing techniques used for this projects are:\n",
    "1. Normalization\n",
    "2. Frame Blocking\n",
    "3. Windowing\n",
    "4. Fast Fourier Transform (FFT)\n",
    "5. Mel Filterbank\n",
    "6. Discrete Cosine Transform (DCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is a process of adjusting the range of a signal to a certain range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(audio):\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Frame Blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame blocking is a process of splitting speech signal into a series of frames with equal length. Usually, the frame length is 20-40 ms [2]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_blocking(audio, sample_rate, FFT_SIZE, hop_size):\n",
    "    audio = np.pad(audio, FFT_SIZE // 2, mode = \"reflect\")\n",
    "\n",
    "    frame_len = int(np.round(sample_rate * hop_size / 1000))\n",
    "\n",
    "    frame_num = (len(audio) - FFT_SIZE) // frame_len + 1\n",
    "\n",
    "    frames = np.zeros((frame_num, FFT_SIZE))\n",
    "\n",
    "    for i in range(frame_num):\n",
    "        frames[i] = audio[i * frame_len : i * frame_len + FFT_SIZE]\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Windowing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windowing is a process to help smoothing the signal and avoiding signal discontinuity [2]. Windowing techniques are divided into:\n",
    "- Rectangular Window\n",
    "- Hamming Window\n",
    "- Hanning Window\n",
    "- Blackman Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowing(frames, FFT_SIZE, windowing_techniques = \"hann\"):\n",
    "    window = scipy.signal.get_window(windowing_techniques, FFT_SIZE)\n",
    "    audio_window = frames * window\n",
    "\n",
    "    return audio_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Fast Fourier Transform (FFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast Fourier Transform (FFT) is a process of converting signal from time domain into frequency domain [3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFT(audio_window, FFT_SIZE):\n",
    "    audio_fft = np.empty((audio_window.shape[0], 1 + FFT_SIZE // 2))\n",
    "\n",
    "    for i in range(audio_window.shape[0]):\n",
    "        audio_fft[i] = scipy.fftpack.fft(audio_window[i])[:audio_fft.shape[1]]\n",
    "\n",
    "    return audio_fft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Mel Filterbank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mel filterbank is a process to approximate the non-linear human auditory system's frequency response [4]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_to_mel(frequency):\n",
    "    return 2595 * np.log10(1 + (frequency / 700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_to_frequency(mel):\n",
    "    return 700 * (10 ** (mel / 2595) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filter_points(frequency_min, frequency_max, mel_filter_num, FFT_SIZE, sample_rate):\n",
    "    mel_min = frequency_to_mel(frequency_min)\n",
    "    mel_max = frequency_to_mel(frequency_max)\n",
    "    \n",
    "    mel = np.linspace(mel_min, mel_max, mel_filter_num + 2)\n",
    "    \n",
    "    frequency = mel_to_frequency(mel)\n",
    "\n",
    "    return np.floor((FFT_SIZE + 1) / sample_rate * frequency).astype(int), frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filters(filter_points, FFT_SIZE):\n",
    "    filters = np.zeros((len(filter_points) - 2, int(FFT_SIZE) // 2 + 1))\n",
    "    \n",
    "    for i in range(filters.shape[0]):\n",
    "        filters[i, filter_points[i]:filter_points[i + 1]] = np.linspace(0, 1, filter_points[i + 1] - filter_points[i])\n",
    "        filters[i, filter_points[i + 1]:filter_points[i + 2]] = np.linspace(1, 0, filter_points[i + 2] - filter_points[i + 1])\n",
    "        \n",
    "    return filters        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_filterbank(sample_rate, frequency_min, frequency_max, mel_filter_num, FFT_SIZE):\n",
    "    filter_points, frequency = get_filter_points(frequency_min, frequency_max, mel_filter_num, FFT_SIZE, sample_rate)\n",
    "    filters = get_filters(filter_points, FFT_SIZE)\n",
    "\n",
    "    return filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Discrete Cosine Transform (DCT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrete Cosine Transform (DCT) is a process to decorrelate the filterbank energies and obtain a compact representation of the spectral envelope of the logarithmically-scaled filterbank [4]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DCT(filter_num, filter_len):\n",
    "    basis = np.empty((filter_num, filter_len))\n",
    "    \n",
    "    basis[0, :] = 1 / np.sqrt(filter_len)\n",
    "    \n",
    "    samples = np.arange(1, 2 * filter_len, 2) * np.pi / (filter_len * 2)\n",
    "    \n",
    "    for i in range(1, filter_num):\n",
    "        basis[i, :] = np.cos(i * samples) / np.sqrt(filter_len * 2)\n",
    "        \n",
    "    return basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature extraction technique used for this project is the Mel Frequency Cepstral Coefficient (MFCC). MFCC is one of the most commonly used feature extraction in speech recognition because it can work well on inputs with a high level of correlation [5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MFCC(audio, sample_rate, FFT_SIZE = 512, hop_size = 10, mel_filter_num = 26, frequency_min = 0, frequency_max = None, num_coefficients = 13):\n",
    "    if frequency_max is None:\n",
    "        frequency_max = sample_rate // 2  #\n",
    "    \n",
    "    audio = normalization(audio)\n",
    "\n",
    "    frames = frame_blocking(audio, sample_rate, FFT_SIZE, hop_size)\n",
    "\n",
    "    audio_window = windowing(frames, FFT_SIZE)\n",
    "\n",
    "    audio_fft = FFT(audio_window, FFT_SIZE)\n",
    "\n",
    "    filters = mel_filterbank(sample_rate, frequency_min, frequency_max, mel_filter_num, FFT_SIZE)\n",
    "    \n",
    "    power_spectrum = np.abs(audio_fft) ** 2 \n",
    "    \n",
    "    mel_spectrum = np.dot(power_spectrum, filters.T)\n",
    "\n",
    "    mel_spectrum_log = np.log(mel_spectrum + 1e-6) \n",
    "\n",
    "    mfcc = DCT(mel_filter_num, mel_spectrum_log.shape[1])\n",
    "\n",
    "    return mfcc.dot(mel_spectrum_log.T).T[:, :num_coefficients] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mfcc_to_df(df, sample_rate = 16000, FFT_SIZE = 512, hop_size = 10, mel_filter_num = 26, frequency_min = 0, frequency_max = None, num_coefficients = 13):\n",
    "    mfccs = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        audio_path = f'../data/cv-corpus-6.1-indonesian/clips/{row[\"path\"]}'\n",
    "        \n",
    "        audio, sr = librosa.load(audio_path, sr = sample_rate)\n",
    "\n",
    "        mfcc = MFCC(audio, sr, FFT_SIZE, hop_size, mel_filter_num, frequency_min, frequency_max, num_coefficients)\n",
    "        \n",
    "        mfccs.append(mfcc)\n",
    "    \n",
    "    df[\"mfcc\"] = mfccs\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_7144\\3319919952.py:5: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  audio_fft[i] = scipy.fftpack.fft(audio_window[i])[:audio_fft.shape[1]]\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_7144\\3319919952.py:5: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  audio_fft[i] = scipy.fftpack.fft(audio_window[i])[:audio_fft.shape[1]]\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_7144\\3319919952.py:5: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  audio_fft[i] = scipy.fftpack.fft(audio_window[i])[:audio_fft.shape[1]]\n"
     ]
    }
   ],
   "source": [
    "train_df = apply_mfcc_to_df(train_df)\n",
    "valid_df = apply_mfcc_to_df(valid_df)\n",
    "test_df = apply_mfcc_to_df(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model used for this projects are:\n",
    "- XLSR-53\n",
    "- Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch CUDA Available: True\n",
      "PyTorch CUDA Version: 11.8\n",
      "PyTorch cuDNN Enabled: True\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"PyTorch CUDA Version:\", torch.version.cuda)\n",
    "print(\"PyTorch cuDNN Enabled:\", torch.backends.cudnn.enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"mfcc\"] = train_df[\"mfcc\"].apply(lambda x: torch.tensor(x, dtype = torch.float32))\n",
    "valid_df[\"mfcc\"] = valid_df[\"mfcc\"].apply(lambda x: torch.tensor(x, dtype = torch.float32))\n",
    "test_df[\"mfcc\"] = test_df[\"mfcc\"].apply(lambda x: torch.tensor(x, dtype = torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ`~!@#$%^&*()-_+=|\\\\]}[{'\\\":;/?.>,< \" \n",
    "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
    "char_to_idx[\"<UNK>\"] = len(char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'b': 1,\n",
       " 'c': 2,\n",
       " 'd': 3,\n",
       " 'e': 4,\n",
       " 'f': 5,\n",
       " 'g': 6,\n",
       " 'h': 7,\n",
       " 'i': 8,\n",
       " 'j': 9,\n",
       " 'k': 10,\n",
       " 'l': 11,\n",
       " 'm': 12,\n",
       " 'n': 13,\n",
       " 'o': 14,\n",
       " 'p': 15,\n",
       " 'q': 16,\n",
       " 'r': 17,\n",
       " 's': 18,\n",
       " 't': 19,\n",
       " 'u': 20,\n",
       " 'v': 21,\n",
       " 'w': 22,\n",
       " 'x': 23,\n",
       " 'y': 24,\n",
       " 'z': 25,\n",
       " 'A': 26,\n",
       " 'B': 27,\n",
       " 'C': 28,\n",
       " 'D': 29,\n",
       " 'E': 30,\n",
       " 'F': 31,\n",
       " 'G': 32,\n",
       " 'H': 33,\n",
       " 'I': 34,\n",
       " 'J': 35,\n",
       " 'K': 36,\n",
       " 'L': 37,\n",
       " 'M': 38,\n",
       " 'N': 39,\n",
       " 'O': 40,\n",
       " 'P': 41,\n",
       " 'Q': 42,\n",
       " 'R': 43,\n",
       " 'S': 44,\n",
       " 'T': 45,\n",
       " 'U': 46,\n",
       " 'V': 47,\n",
       " 'W': 48,\n",
       " 'X': 49,\n",
       " 'Y': 50,\n",
       " 'Z': 51,\n",
       " '`': 52,\n",
       " '~': 53,\n",
       " '!': 54,\n",
       " '@': 55,\n",
       " '#': 56,\n",
       " '$': 57,\n",
       " '%': 58,\n",
       " '^': 59,\n",
       " '&': 60,\n",
       " '*': 61,\n",
       " '(': 62,\n",
       " ')': 63,\n",
       " '-': 64,\n",
       " '_': 65,\n",
       " '+': 66,\n",
       " '=': 67,\n",
       " '|': 68,\n",
       " '\\\\': 69,\n",
       " ']': 70,\n",
       " '}': 71,\n",
       " '[': 72,\n",
       " '{': 73,\n",
       " \"'\": 74,\n",
       " '\"': 75,\n",
       " ':': 76,\n",
       " ';': 77,\n",
       " '/': 78,\n",
       " '?': 79,\n",
       " '.': 80,\n",
       " '>': 81,\n",
       " ',': 82,\n",
       " '<': 83,\n",
       " ' ': 84,\n",
       " '<UNK>': 85}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, max_len=None):\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mfcc = self.df.iloc[idx][\"mfcc\"]\n",
    "        label = self.df.iloc[idx][\"sentence\"]\n",
    "        label = \"\".join([char if char in char_to_idx else \"<UNK>\" for char in label])\n",
    "\n",
    "        label_indices = torch.tensor([char_to_idx[char] for char in label], dtype = torch.long)\n",
    " \n",
    "        mfcc_tensor = torch.tensor(mfcc, dtype = torch.float32)\n",
    "\n",
    "        return {\"mfcc\": mfcc_tensor, \"label\": label_indices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    mfccs = [item[\"mfcc\"] for item in batch]\n",
    "    labels = [item[\"label\"] for item in batch]\n",
    "\n",
    "    mfccs_padded = torch.nn.utils.rnn.pad_sequence(mfccs, batch_first = True, padding_value = 0)\n",
    "    label_lengths = torch.tensor([len(label) for label in labels], dtype = torch.long)\n",
    "\n",
    "    labels_padded = torch.nn.utils.rnn.pad_sequence(labels, batch_first = True, padding_value = 0)\n",
    "\n",
    "    return {\"mfcc\": mfccs_padded, \"label\": labels_padded, \"label_lengths\": label_lengths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_df)\n",
    "valid_dataset = Dataset(valid_df)\n",
    "test_dataset = Dataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 4, shuffle = True, collate_fn = collate_fn)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = 4, shuffle = False, collate_fn = collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 4, shuffle = True, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, ctc_loss):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_values = batch[\"mfcc\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        label_lengths = batch[\"label_lengths\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input_values)\n",
    "\n",
    "        output = output.transpose(0, 1)\n",
    "\n",
    "        loss = ctc_loss(output, labels, input_lengths = torch.full((input_values.size(0),), output.size(0)), target_lengths = label_lengths)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valid_loader, ctc_loss):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            input_values = batch[\"mfcc\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            label_lengths = batch[\"label_lengths\"].to(device)\n",
    "\n",
    "            output = model(input_values)\n",
    "\n",
    "            output = output.transpose(0, 1)\n",
    "\n",
    "            loss = ctc_loss(output, labels, input_lengths=torch.full((input_values.size(0),), output.size(0)), target_lengths = label_lengths)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. XLSR-53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XLSR-53 is a pretrained model built on wav2vec 2.0 thas has been trained in 53 different languages. There are four importance elements, which are Feature Encoder, Quantization Module, Context Network, and Pretraining and Contrasive Loss [6]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![XLSR-53 Architecture](../assets/xlsr-53.png)\n",
    "\n",
    "Fig. 1. XLSR-53 Architecture [6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this project, the input layer of XLSR-53 is modified so that it can accept MFCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\main-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLSR53(torch.nn.Module):\n",
    "    def __init__(self, model, n_mfcc = 13, hidden_size = 1024):\n",
    "        super().__init__()\n",
    "        self.wav2vec2 = model\n",
    "        self.mfcc_projection = torch.nn.Linear(n_mfcc, hidden_size)\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        input_values = self.mfcc_projection(input_values)\n",
    "        output = self.wav2vec2.wav2vec2.encoder(input_values).last_hidden_state\n",
    "        output = torch.nn.functional.log_softmax(output, dim = -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsr53 = XLSR53(Wav2Vec2ForPreTraining.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLSR53(\n",
       "  (wav2vec2): Wav2Vec2ForPreTraining(\n",
       "    (wav2vec2): Wav2Vec2Model(\n",
       "      (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): Wav2Vec2LayerNormConvLayer(\n",
       "            (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (feature_projection): Wav2Vec2FeatureProjection(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "        (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "          (conv): ParametrizedConv1d(\n",
       "            1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (padding): Wav2Vec2SamePadLayer()\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "            (attention): Wav2Vec2SdpaAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): Wav2Vec2FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dropout_features): Dropout(p=0.0, inplace=False)\n",
       "    (quantizer): Wav2Vec2GumbelVectorQuantizer(\n",
       "      (weight_proj): Linear(in_features=512, out_features=640, bias=True)\n",
       "    )\n",
       "    (project_hid): Linear(in_features=1024, out_features=768, bias=True)\n",
       "    (project_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (mfcc_projection): Linear(in_features=13, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlsr53.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsr53_ctc_loss = torch.nn.CTCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsr53_optimizer = torch.optim.Adam(xlsr53.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |   6500 MiB |   7875 MiB | 640056 MiB | 633555 MiB |\\n|       from large pool |   6492 MiB |   7865 MiB | 632340 MiB | 625848 MiB |\\n|       from small pool |      8 MiB |     12 MiB |   7716 MiB |   7707 MiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |   6500 MiB |   7875 MiB | 640056 MiB | 633555 MiB |\\n|       from large pool |   6492 MiB |   7865 MiB | 632340 MiB | 625848 MiB |\\n|       from small pool |      8 MiB |     12 MiB |   7716 MiB |   7707 MiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |   6488 MiB |   7869 MiB | 637404 MiB | 630916 MiB |\\n|       from large pool |   6480 MiB |   7859 MiB | 629693 MiB | 623213 MiB |\\n|       from small pool |      8 MiB |     12 MiB |   7711 MiB |   7702 MiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |   7198 MiB |  10776 MiB |  10776 MiB |   3578 MiB |\\n|       from large pool |   7188 MiB |  10762 MiB |  10762 MiB |   3574 MiB |\\n|       from small pool |     10 MiB |     14 MiB |     14 MiB |      4 MiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory | 713915 KiB |   1843 MiB | 475327 MiB | 474630 MiB |\\n|       from large pool | 712368 KiB |   1840 MiB | 467394 MiB | 466698 MiB |\\n|       from small pool |   1547 KiB |      6 MiB |   7933 MiB |   7931 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |    1560    |    2002    |  112128    |  110568    |\\n|       from large pool |     689    |     744    |   64197    |   63508    |\\n|       from small pool |     871    |    1264    |   47931    |   47060    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |    1560    |    2002    |  112128    |  110568    |\\n|       from large pool |     689    |     744    |   64197    |   63508    |\\n|       from small pool |     871    |    1264    |   47931    |   47060    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     323    |     406    |     406    |      83    |\\n|       from large pool |     318    |     399    |     399    |      81    |\\n|       from small pool |       5    |       7    |       7    |       2    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |     132    |     189    |   58849    |   58717    |\\n|       from large pool |     117    |     176    |   42547    |   42430    |\\n|       from small pool |      15    |      64    |   16302    |   16287    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device = None, abbreviated = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "patience = 3\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_7144\\2266708239.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mfcc_tensor = torch.tensor(mfcc, dtype = torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 19.6626, Validation Loss: 12.8608\n",
      "Epoch 2/20 - Train Loss: 8.5388, Validation Loss: 7.2756\n",
      "Epoch 3/20 - Train Loss: 5.8260, Validation Loss: 5.5134\n",
      "Epoch 4/20 - Train Loss: 4.7475, Validation Loss: 4.6750\n",
      "Epoch 5/20 - Train Loss: 3.9206, Validation Loss: 4.2789\n",
      "Epoch 6/20 - Train Loss: 3.3557, Validation Loss: 4.1368\n",
      "Epoch 7/20 - Train Loss: 2.8991, Validation Loss: 4.1675\n",
      "Epoch 8/20 - Train Loss: 2.6377, Validation Loss: 3.9332\n",
      "Epoch 9/20 - Train Loss: 2.4177, Validation Loss: 4.0265\n",
      "Epoch 10/20 - Train Loss: 2.2358, Validation Loss: 3.8659\n",
      "Epoch 11/20 - Train Loss: 2.1321, Validation Loss: 3.8594\n",
      "Epoch 12/20 - Train Loss: 1.9989, Validation Loss: 3.7178\n",
      "Epoch 13/20 - Train Loss: 1.9005, Validation Loss: 3.8757\n",
      "Epoch 14/20 - Train Loss: 1.8206, Validation Loss: 2.9413\n",
      "Epoch 15/20 - Train Loss: 1.7607, Validation Loss: 3.1276\n",
      "Epoch 16/20 - Train Loss: 1.7080, Validation Loss: 3.0639\n",
      "Epoch 17/20 - Train Loss: 1.7202, Validation Loss: 2.7201\n",
      "Epoch 18/20 - Train Loss: 1.6345, Validation Loss: 2.7608\n",
      "Epoch 19/20 - Train Loss: 1.6042, Validation Loss: 2.2580\n",
      "Epoch 20/20 - Train Loss: 1.5710, Validation Loss: 2.1743\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(xlsr53, train_loader, xlsr53_optimizer, xlsr53_ctc_loss)\n",
    "    val_loss = evaluate(xlsr53, valid_loader, xlsr53_ctc_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0 \n",
    "\n",
    "        torch.save({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model_state_dict\": xlsr53.state_dict(),\n",
    "            \"optimizer_state_dict\": xlsr53_optimizer.state_dict(),\n",
    "            \"loss\": val_loss\n",
    "        }, \"../models/best_model.pth\")\n",
    "        \n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] L. Maison and Y. Estève, “Some voices are too common: Building fair speech recognition systems using the Common Voice dataset.”\n",
    "\n",
    "[2] M. Labied, A. Belangour, M. Banane, and A. Erraissi, “An overview of Automatic Speech Recognition Preprocessing Techniques,” in 2022 International Conference on Decision Aid Sciences and Applications (DASA), IEEE, Mar. 2022, pp. 804–809. doi: 10.1109/DASA54658.2022.9765043.\n",
    "\n",
    "[3] R. D. Septiawan, P. R. Rayes, and N. A. Robbaniyyah, “Simulasi Penghilangan Noise pada Sinyal Suara menggunakan Metode Fast Fourier Transfrom,” Semeton Mathematics Journal, vol. 1, no. 1, pp. 1–7, Apr. 2024, doi: 10.29303/semeton.v1i1.203.\n",
    "\n",
    "[4] M. Y. Wang, Z. Chu, C. Entzminger, Y. Ding, and Q. Zhang, “Visualization and Interpretation of Mel-Frequency Cepstral Coefficients for UAV Drone Audio Data,” in Proceedings of the 13th International Conference on Data Science, Technology and Applications, DATA 2024, SciTePress, 2024, pp. 528–534. doi: 10.5220/0012827400003756.\n",
    "  \n",
    "[5] W. Mustikarini, R. Hidayat, and A. Bejo, “Real-Time Indonesian Language Speech Recognition with MFCC Algorithms and Python-Based SVM,” 2019.\n",
    "\n",
    "[6] P. Arisaputra and A. Zahra, “Indonesian Automatic Speech Recognition with XLSR-53,” Ingénierie des systèmes d information, vol. 27, no. 6, pp. 973–982, Dec. 2022, doi: 10.18280/isi.270614.\n",
    "  \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
